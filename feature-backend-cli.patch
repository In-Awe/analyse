*** Begin Patch
*** Add File: .gitignore
+ # general
+ .DS_Store
+ __pycache__/
+ *.pyc
+
+ # secrets and env
+ .env
+ backend/config.env
+
+ # model checkpoints
+ backend/model/checkpoints/
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/python-ci.yml
+name: Python CI
+on: [push, pull_request]
+jobs:
+  test:
+    runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        python-version: [3.9]
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: ${{ matrix.python-version }}
+      - name: Install dependencies
+        run: |
+          python -m pip install --upgrade pip
+          pip install -r backend/requirements.txt
+      - name: Run tests
+        run: pytest backend/tests -q
*** End Patch
*** Begin Patch
*** Add File: backend/README.md
+Backend for LLM trading assistant
+=================================
+
+This folder contains a Python CLI and supporting modules to:
+
+- ingest Binance CSVs
+- compute technical indicators
+- run local LLM inference (quantised when possible)
+- run a toy LoRA training job
+- fetch historical klines from Binance
+- run a naive backtest
+
+Quickstart (macOS)
+------------------
+
+1. Create and activate a virtual environment
+
+```bash
+cd backend
+python3 -m venv .venv
+source .venv/bin/activate
+python -m pip install --upgrade pip setuptools wheel
+pip install -r requirements.txt
+```
+
+2. Set environment variables (use `backend/config.example.env` as a template)
+
+```bash
+export BINANCE_API_KEY=...
+export BINANCE_API_SECRET=...
+export MODEL_REPO=meta-llama/Llama-2-7b-chat-hf
+```
+
+3. Run CLI examples
+
+```bash
+# ingest a CSV and print a summary
+python cli.py ingest --path /mnt/data/BTCUSDT_1m_January_2025.csv
+
+# analyse last 60 rows using the local model
+python cli.py analyze --path /mnt/data/BTCUSDT_1m_January_2025.csv --lookback 60
+
+# fetch historical data from Binance and save to CSV
+python cli.py fetch --symbol BTCUSDT --start 2025-01-01 --end 2025-02-01 --out-csv btc_jan.csv
+
+# run a toy LoRA training job
+python cli.py train --dataset backend/samples/toy_train.json --output-dir backend/model/checkpoints
+```
+
+Notes
+-----
+
+- If you are on Apple Silicon, make sure PyTorch MPS support is installed. Check with:
+  `python -c "import torch; print(torch.backends.mps.is_available())"`.
+- bitsandbytes can be problematic to install on macOS. If installation fails, use CPU fallback. See `backend/model/infer.py` for graceful fallback logic.
+- Do not commit real API keys. Use `backend/config.example.env` and a local `.env` file or environment variables.
+
+Project layout
+--------------
+
+```
+backend/
+├─ README.md
+├─ requirements.txt
+├─ config.example.env
+├─ cli.py
+├─ data/
+│  ├─ __init__.py
+│  ├─ loader.py
+│  └─ features.py
+├─ binance_client.py
+├─ model/
+│  ├─ __init__.py
+│  ├─ infer.py
+│  └─ lora_train.py
+├─ backtest/
+│  ├─ __init__.py
+│  ├─ engine.py
+│  └─ metrics.py
+└─ tests/
+   ├─ test_loader.py
+   └─ test_binance_client.py
+```
+
+If you want a full patch applied automatically, use the accompanying git patch.
*** End Patch
*** Begin Patch
*** Add File: backend/requirements.txt
+typer[all]==0.9.0
+pandas==2.2.0
+numpy==1.26.0
+pandas-ta==0.3.0b0
+python-binance==1.0.16
+transformers==4.34.0
+accelerate==0.22.0
+peft==0.4.0
+bitsandbytes==0.39.0
+torch==2.2.0
+torchaudio==2.2.0
+pytest==7.4.0
+rich==13.6.0
+python-dotenv==1.0.0
+faiss-cpu==1.7.4
+
+# notes: bitsandbytes and torch versions change frequently. If install fails on macOS, use CPU fallback.
+
+# analysis & feature libs
+statsmodels==0.14.0
+scipy==1.11.3
+scikit-learn==1.3.2
+# xgboost is optional but valuable for importance ranking; may be heavy on macOS
+xgboost==1.8.1
*** End Patch
*** Begin Patch
*** Add File: backend/config.example.env
+# backend/config.example.env
+BINANCE_API_KEY=
+BINANCE_API_SECRET=
+BINANCE_TESTNET_API_KEY=
+BINANCE_TESTNET_API_SECRET=
+MODEL_REPO=meta-llama/Llama-2-7b-chat-hf
*** End Patch
*** Begin Patch
*** Add File: backend/cli.py
+#!/usr/bin/env python3
+"""
+backend/cli.py
+Entry point for the CLI using Typer.
+"""
+import os
+import typer
+from rich import print
+
+from backend.data.loader import load_csv, df_to_context
+from backend.binance_client import BinanceClient
+from backend.model.infer import analyse_series
+from backend.model.lora_train import train_lora
+from backend.data.audit import DataAuditor
+
+app = typer.Typer(help="CLI for LLM trading assistant")
+
+
+@app.command()
+def ingest(path: str):
+    """
+    Load a CSV and print basic info
+    """
+    df = load_csv(path)
+    print(f"[green]Loaded[/green] {len(df)} rows from {path}")
+    print(df.tail(3).to_string())
+
+
+@app.command()
+def analyze(path: str, lookback: int = 60):
+    """
+    Analyse last N rows using local LLM
+    """
+    df = load_csv(path)
+    context = df_to_context(df, lookback)
+    out = analyse_series(context)
+    print(out)
+
+
+@app.command()
+def fetch(symbol: str, start: str, end: str, out_csv: str = "fetched.csv"):
+    """
+    Fetch historical klines from Binance and save to CSV
+    """
+    client = BinanceClient()
+    df = client.get_historical_klines(symbol, start, end)
+    df.to_csv(out_csv, index=False)
+    print(f"[green]Saved[/green] {len(df)} rows to {out_csv}")
+
+
+@app.command()
+def train(dataset: str = "backend/samples/toy_train.json", output_dir: str = "backend/model/checkpoints"):
+    """
+    Run a toy LoRA training job for proof of concept
+    """
+    train_lora(dataset, output_dir)
+    print("[green]LoRA training finished (toy run)[/green]")
+
+
+@app.command()
+def audit(path: str, out: str = "backend/reports", freq: str = "1T", impute: str = "ffill"):
+    """
+    Run data audit (gaps, dupes, spikes, imputation, ADF, ACF/PACF, intraday profile).
+    Writes a JSON report to `out`.
+    """
+    df = load_csv(path)
+    auditor = DataAuditor(freq=freq)
+    result = auditor.audit(df, impute_method=impute)
+    report_path = auditor.export_report(result["report"], out_dir=out)
+    print(f"[green]Audit saved to[/green] {report_path}")
+
+
+if __name__ == "__main__":
+    app()
*** End Patch
*** Begin Patch
*** Add File: backend/data/__init__.py
+# data package
+__all__ = ["loader", "features", "audit"]
+
+from . import audit  # re-export for convenience
*** End Patch
*** Begin Patch
*** Add File: backend/data/loader.py
+import pandas as pd
+from .features import add_indicators
+
+def load_csv(path: str) -> pd.DataFrame:
+    """
+    Expect CSV with at least: timestamp, open, high, low, close, volume
+    Timestamps will be parsed to pandas datetime
+    """
+    df = pd.read_csv(path)
+    # support common column variants
+    if 'timestamp' not in df.columns and 'open_time' in df.columns:
+        df = df.rename(columns={'open_time':'timestamp'})
+    if 'timestamp' in df.columns:
+        df['timestamp'] = pd.to_datetime(df['timestamp'])
+    else:
+        # attempt epoch ms fallback
+        if 'open_time' in df.columns:
+            df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')
+        else:
+            raise ValueError("CSV missing timestamp or open_time")
+    df = df.sort_values('timestamp').reset_index(drop=True)
+    # coerce numeric columns
+    for c in ['open','high','low','close','volume']:
+        if c in df.columns:
+            df[c] = pd.to_numeric(df[c], errors='coerce')
+    df = add_indicators(df)
+    return df
+
+def df_to_context(df: pd.DataFrame, lookback: int = 60) -> list:
+    """
+    Return last `lookback` rows as list of dicts for the prompt builder
+    """
+    recent = df.tail(lookback).copy()
+    cols = ['timestamp','open','high','low','close','volume','rsi','ema_50','ema_200']
+    available = [c for c in cols if c in recent.columns]
+    records = recent[available].to_dict('records')
+    return records
*** End Patch
*** Begin Patch
*** Add File: backend/data/features.py
+import pandas as pd
+import pandas_ta as ta
+
+def add_indicators(df: pd.DataFrame) -> pd.DataFrame:
+    """
+    Add simple indicators: RSI(14), EMA50, EMA200
+    If pandas_ta fails, leave NaN values.
+    """
+    if 'close' not in df.columns:
+        return df
+    try:
+        df['rsi'] = ta.rsi(df['close'], length=14)
+        df['ema_50'] = ta.ema(df['close'], length=50)
+        df['ema_200'] = ta.ema(df['close'], length=200)
+    except Exception:
+        df['rsi'] = pd.NA
+        df['ema_50'] = pd.NA
+        df['ema_200'] = pd.NA
+    return df
*** End Patch
*** Begin Patch
*** Add File: backend/data/audit.py
+"""
+backend/data/audit.py
+
+DataAuditor: dataset validation, gap/duplicate detection, spike detection,
+imputation strategies, statistical characterization (log returns, ADF),
+ACF/PACF for returns and squared returns, intraday seasonality profiling,
+and report export.
+"""
+from __future__ import annotations
+
+import os
+from typing import Dict, Any, Optional, Tuple
+
+import numpy as np
+import pandas as pd
+
+try:
+    from statsmodels.tsa.stattools import adfuller, acf, pacf
+except Exception:
+    adfuller = acf = pacf = None
+
+
+class DataAuditor:
+    def __init__(self, freq: str = "1T"):
+        """
+        freq: expected pandas frequency string for regular timestamps ("1T" = 1 minute)
+        """
+        self.freq = freq
+
+    def detect_duplicates(self, df: pd.DataFrame, subset: Optional[list] = None) -> pd.Series:
+        """Return boolean mask of duplicated rows (first occurrence kept)."""
+        subset = subset or ["timestamp", "open", "high", "low", "close", "volume"]
+        return df.duplicated(subset=subset, keep="first")
+
+    def detect_gaps(self, df: pd.DataFrame) -> pd.DataFrame:
+        """
+        Detect missing timestamps assuming df['timestamp'] is datetime and sorted.
+        Returns a DataFrame with missing intervals (start, end, expected_count).
+        """
+        if "timestamp" not in df.columns:
+            raise ValueError("DataFrame must contain 'timestamp' column")
+        ts = pd.to_datetime(df["timestamp"]).sort_values().reset_index(drop=True)
+        full_range = pd.date_range(start=ts.iloc[0], end=ts.iloc[-1], freq=self.freq)
+        missing = full_range.difference(ts)
+        if len(missing) == 0:
+            return pd.DataFrame(columns=["missing_ts"])
+        return pd.DataFrame({"missing_ts": missing})
+
+    def detect_spikes(self, df: pd.DataFrame, column: str = "close", window: int = 20, z_thresh: float = 8.0) -> pd.Series:
+        """
+        Simple spike detector using rolling median and rolling MAD-derived robust std.
+        Returns boolean Series where True marks a spike.
+        """
+        if column not in df.columns:
+            return pd.Series([False] * len(df), index=df.index)
+
+        vals = pd.to_numeric(df[column], errors="coerce").fillna(method="ffill").fillna(method="bfill")
+        roll_med = vals.rolling(window=window, min_periods=1, center=False).median()
+        # robust scale via MAD
+        mad = (vals - roll_med).abs().rolling(window=window, min_periods=1).median()
+        # approximate std from MAD: std ≈ 1.4826 * MAD
+        roll_std = mad * 1.4826
+        roll_std = roll_std.replace(0, np.nan).fillna(method="bfill").fillna(method="ffill").fillna(1e-8)
+        z = (vals - roll_med).abs() / roll_std
+        return z > z_thresh
+
+    def impute(self, df: pd.DataFrame, method: str = "ffill", limit: Optional[int] = None) -> Tuple[pd.DataFrame, pd.Series]:
+        """
+        Impute missing numeric values in ['open','high','low','close','volume'].
+        method: 'ffill', 'bfill', 'linear'
+        Returns (df_imputed, is_imputed_mask)
+        """
+        numeric_cols = [c for c in ["open", "high", "low", "close", "volume"] if c in df.columns]
+        df2 = df.copy()
+        is_nan = df2[numeric_cols].isna().any(axis=1)
+        if method == "linear":
+            df2[numeric_cols] = df2[numeric_cols].interpolate(method="linear", limit=limit, limit_direction="both")
+        elif method in ("ffill", "bfill"):
+            df2[numeric_cols] = df2[numeric_cols].fillna(method=method, limit=limit)
+        else:
+            raise ValueError("unsupported imputation method")
+        # for any remaining NaNs, fill with nearest value
+        df2[numeric_cols] = df2[numeric_cols].fillna(method="ffill").fillna(method="bfill")
+        is_imputed = is_nan & df2[numeric_cols].notna().any(axis=1)
+        is_imputed = is_imputed.astype(bool)
+        # add column
+        df2["is_imputed_datapoint"] = is_imputed
+        return df2, is_imputed
+
+    def compute_log_returns(self, df: pd.DataFrame, col: str = "close", out_col: str = "log_return") -> pd.DataFrame:
+        """Add log return column."""
+        df2 = df.copy()
+        if col not in df2.columns:
+            df2[out_col] = np.nan
+            return df2
+        df2[out_col] = np.log(pd.to_numeric(df2[col], errors="coerce")).diff()
+        return df2
+
+    def adf_test(self, series: pd.Series) -> Dict[str, Any]:
+        """Run Augmented Dickey-Fuller test; return results dict (or empty if statsmodels missing)."""
+        if adfuller is None:
+            return {"error": "statsmodels not installed"}
+        series_clean = series.dropna()
+        if len(series_clean) < 10:
+            return {"error": "not enough data for ADF"}
+        res = adfuller(series_clean, autolag="AIC")
+        return {
+            "adf_stat": float(res[0]),
+            "pvalue": float(res[1]),
+            "usedlag": int(res[2]),
+            "nobs": int(res[3]),
+            "crit_vals": {k: float(v) for k, v in res[4].items()},
+            "icbest": float(res[5]),
+        }
+
+    def compute_acf_pacf(self, series: pd.Series, nlags: int = 40) -> Dict[str, Any]:
+        """Return ACF and PACF arrays (or error if statsmodels missing)."""
+        if acf is None or pacf is None:
+            return {"error": "statsmodels not installed"}
+        s = series.dropna()
+        if len(s) < 2:
+            return {"acf": [], "pacf": []}
+        a = acf(s, nlags=nlags, fft=True, missing="drop")
+        p = pacf(s, nlags=nlags, method="ywunbiased")
+        return {"acf": a.tolist(), "pacf": p.tolist()}
+
+    def intraday_profile(self, df: pd.DataFrame, return_col: str = "log_return") -> pd.DataFrame:
+        """
+        Compute intraday seasonality profiles:
+        - avg volume by hour/minute
+        - avg absolute return (vol) by hour/minute
+        Returns DataFrame indexed by hour,minute with columns avg_volume, avg_abs_ret.
+        """
+        df2 = df.copy()
+        df2["timestamp"] = pd.to_datetime(df2["timestamp"])
+        df2["hour"] = df2["timestamp"].dt.hour
+        df2["minute"] = df2["timestamp"].dt.minute
+        profile = (
+            df2.groupby(["hour", "minute"])
+            .agg(avg_volume=("volume", "mean"), avg_abs_ret=(return_col, lambda s: s.abs().mean()))
+            .reset_index()
+        )
+        return profile
+
+    def audit(self, df: pd.DataFrame, impute_method: Optional[str] = "ffill", impute_limit: Optional[int] = None) -> Dict[str, Any]:
+        """
+        Run a full audit and return a report dict. This does not mutate the input df.
+        """
+        report: Dict[str, Any] = {}
+        if "timestamp" in df.columns:
+            report["n_rows"] = int(len(df))
+            # duplicates
+            dup_mask = self.detect_duplicates(df)
+            report["n_duplicates"] = int(dup_mask.sum())
+            report["duplicate_sample"] = df[dup_mask].head(5).to_dict("records")
+            # gaps
+            gaps = self.detect_gaps(df)
+            report["n_missing_timestamps"] = int(len(gaps))
+            if len(gaps) > 0:
+                report["missing_sample"] = gaps.head(5).to_dict("records")
+        else:
+            report["warning"] = "no timestamp column"
+
+        # spike detection on close/open/high/low
+        spikes = {}
+        for c in ["close", "open", "high", "low"]:
+            if c in df.columns:
+                mask = self.detect_spikes(df, column=c)
+                spikes[c] = {"n_spikes": int(mask.sum()), "sample_idx": df.index[mask].tolist()[:5]}
+        report["spikes"] = spikes
+
+        # impute
+        df_imputed, is_imputed = self.impute(df, method=impute_method or "ffill", limit=impute_limit)
+        report["n_imputed_points"] = int(is_imputed.sum())
+        report["impute_method"] = impute_method
+
+        # compute log returns & adf
+        df_lr = self.compute_log_returns(df_imputed)
+        report["log_return_na"] = int(df_lr["log_return"].isna().sum())
+        report["adf_log_return"] = self.adf_test(df_lr["log_return"])
+
+        # acf/pacf for returns and squared returns
+        report["acf_pacf_log_return"] = self.compute_acf_pacf(df_lr["log_return"])
+        report["acf_pacf_sq_log_return"] = self.compute_acf_pacf(df_lr["log_return"].dropna() ** 2)
+
+        # intraday profile
+        try:
+            profile = self.intraday_profile(df_lr, return_col="log_return")
+            # include a small summary
+            top_vol = profile.sort_values("avg_volume", ascending=False).head(3).to_dict("records")
+            top_vol_hours = [f"{r['hour']:02d}:{r['minute']:02d}" for r in top_vol]
+            report["intraday_top_volume_slots"] = top_vol_hours
+            report["intraday_profile_sample"] = profile.head(5).to_dict("records")
+        except Exception as e:
+            report["intraday_error"] = str(e)
+
+        # return full report and also the imputed dataframe for downstream use
+        return {"report": report, "df_imputed": df_imputed, "is_imputed_mask": is_imputed}
+
+    def export_report(self, report: Dict[str, Any], out_dir: str = "backend/reports") -> str:
+        """
+        Save a minimal report to out_dir as JSON and return path.
+        (User can inspect the raw report dict as needed.)
+        """
+        import json
+
+        os.makedirs(out_dir, exist_ok=True)
+        path = os.path.join(out_dir, "data_audit_report.json")
+        with open(path, "w") as fh:
+            json.dump(report, fh, indent=2, default=str)
+        return path
+
+if __name__ == "__main__":  # quick demo
+    import argparse
+    from pathlib import Path
+
+    p = argparse.ArgumentParser()
+    p.add_argument("--csv", required=True)
+    p.add_argument("--out", default="backend/reports")
+    p.add_argument("--freq", default="1T")
+    args = p.parse_args()
+    csv = Path(args.csv)
+    df = pd.read_csv(csv)
+    auditor = DataAuditor(freq=args.freq)
+    result = auditor.audit(df)
+    path = auditor.export_report(result["report"], out_dir=args.out)
+    print("Saved report to", path)
*** End Patch
*** Begin Patch
*** Add File: backend/features/advanced.py
+"""
+backend/features/advanced.py
+
+Vectorized implementations of:
+- VWAP (rolling)
+- ATR
+- Bollinger Bands
+- MACD (+signal)
+- OBV
+- VVR (Volume-to-Volatility Ratio)
+- volatility-of-volatility
+- candlestick morphology (body/total, upper/lower wick ratios)
+- microstructure proxies (rolling price-volume corr, ADV)
+- multi-timescale EMAs
+- interaction features (e.g., RSI * ATR) when inputs available
+
+Also includes build_feature_matrix and feature_report helpers.
+"""
+from __future__ import annotations
+
+import os
+from typing import List, Optional, Tuple
+
+import numpy as np
+import pandas as pd
+
+from sklearn.ensemble import RandomForestClassifier
+
+try:
+    import xgboost as xgb  # optional: used for importance ranking if available
+except Exception:
+    xgb = None
+
+
+def rolling_vwap(df: pd.DataFrame, window: int = 20, price_col: str = "close", vol_col: str = "volume", out_col: str = "vwap") -> pd.DataFrame:
+    """Rolling VWAP (windowed)."""
+    df = df.copy()
+    if price_col not in df.columns or vol_col not in df.columns:
+        df[out_col] = pd.NA
+        return df
+    tp = df[price_col] * df[vol_col]
+    num = tp.rolling(window=window, min_periods=1).sum()
+    den = df[vol_col].rolling(window=window, min_periods=1).sum().replace(0, np.nan).fillna(method="bfill").fillna(method="ffill")
+    df[out_col] = num / den
+    return df
+
+
+def true_range(df: pd.DataFrame) -> pd.Series:
+    """Compute True Range series."""
+    df = df.copy()
+    prev_close = df["close"].shift(1)
+    tr1 = df["high"] - df["low"]
+    tr2 = (df["high"] - prev_close).abs()
+    tr3 = (df["low"] - prev_close).abs()
+    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
+    return tr
+
+
+def atr(df: pd.DataFrame, window: int = 14, out_col: str = "atr") -> pd.DataFrame:
+    df = df.copy()
+    df[out_col] = true_range(df).rolling(window=window, min_periods=1).mean()
+    return df
+
+
+def bollinger_bands(df: pd.DataFrame, window: int = 20, n_std: float = 2.0, price_col: str = "close", prefix: str = "bb") -> pd.DataFrame:
+    df = df.copy()
+    ma = df[price_col].rolling(window=window, min_periods=1).mean()
+    sd = df[price_col].rolling(window=window, min_periods=1).std()
+    df[f"{prefix}_mid"] = ma
+    df[f"{prefix}_upper"] = ma + n_std * sd
+    df[f"{prefix}_lower"] = ma - n_std * sd
+    df[f"{prefix}_width"] = (df[f"{prefix}_upper"] - df[f"{prefix}_lower"]) / df[f"{prefix}_mid"].replace(0, np.nan)
+    return df
+
+
+def ema(df: pd.DataFrame, span: int = 12, col: str = "close", out_col: Optional[str] = None) -> pd.Series:
+    out_col = out_col or f"ema_{span}"
+    return df[col].ewm(span=span, adjust=False).mean().rename(out_col)
+
+
+def macd(df: pd.DataFrame, fast: int = 12, slow: int = 26, signal: int = 9, price_col: str = "close") -> pd.DataFrame:
+    df = df.copy()
+    ema_fast = ema(df, span=fast, col=price_col)
+    ema_slow = ema(df, span=slow, col=price_col)
+    df["macd"] = ema_fast - ema_slow
+    df["macd_signal"] = df["macd"].ewm(span=signal, adjust=False).mean()
+    df["macd_hist"] = df["macd"] - df["macd_signal"]
+    return df
+
+
+def obv(df: pd.DataFrame, price_col: str = "close", vol_col: str = "volume", out_col: str = "obv") -> pd.DataFrame:
+    df = df.copy()
+    sign = np.sign(df[price_col].diff().fillna(0))
+    df[out_col] = (sign * df[vol_col]).fillna(0).cumsum()
+    return df
+
+
+def vvr(df: pd.DataFrame, vol_window: int = 20, ret_col: str = "log_return", vol_col: str = "volume", out_col: str = "vvr") -> pd.DataFrame:
+    """Volume-to-Volatility Ratio: rolling volume / rolling volatility of returns"""
+    df = df.copy()
+    if ret_col not in df.columns or vol_col not in df.columns:
+        df[out_col] = pd.NA
+        return df
+    vol = df[ret_col].rolling(window=vol_window, min_periods=1).std().replace(0, np.nan).fillna(method="bfill").fillna(method="ffill")
+    vol_sum = df[vol_col].rolling(window=vol_window, min_periods=1).mean()
+    df[out_col] = vol_sum / (vol + 1e-12)
+    return df
+
+
+def vol_of_vol(df: pd.DataFrame, ret_col: str = "log_return", window: int = 20, out_col: str = "vol_of_vol") -> pd.DataFrame:
+    df = df.copy()
+    rolling_std = df[ret_col].rolling(window=window, min_periods=1).std()
+    df[out_col] = rolling_std.rolling(window=window, min_periods=1).std()
+    return df
+
+
+def candle_morphology(df: pd.DataFrame, prefix: str = "candle") -> pd.DataFrame:
+    """
+    Compute candle ratios:
+      - body_len / total_len
+      - upper_wick / total_len
+      - lower_wick / total_len
+    """
+    df = df.copy()
+    body = (df["close"] - df["open"]).abs()
+    total = (df["high"] - df["low"]).replace(0, np.nan)
+    upper = df["high"] - df[["open", "close"]].max(axis=1)
+    lower = df[["open", "close"]].min(axis=1) - df["low"]
+    df[f"{prefix}_body_ratio"] = body / total
+    df[f"{prefix}_upper_wick_ratio"] = upper / total
+    df[f"{prefix}_lower_wick_ratio"] = lower / total
+    df[f"{prefix}_body_signed"] = (df["close"] - df["open"]) / (total + 1e-12)
+    df.fillna(0, inplace=True)
+    return df
+
+
+def rolling_price_volume_corr(df: pd.DataFrame, price_col: str = "close", vol_col: str = "volume", window: int = 20, out_col: str = "pv_corr") -> pd.DataFrame:
+    df = df.copy()
+    # correlate absolute returns with volume
+    df["abs_ret"] = df[price_col].pct_change().abs()
+    df[out_col] = df["abs_ret"].rolling(window=window, min_periods=1).corr(df[vol_col])
+    df.drop(columns=["abs_ret"], inplace=True)
+    return df
+
+
+def adv(df: pd.DataFrame, window: int = 1440, vol_col: str = "volume", out_col: str = "adv") -> pd.DataFrame:
+    """Average daily volume approximated by rolling window (default 1440 minutes)."""
+    df = df.copy()
+    df[out_col] = df[vol_col].rolling(window=window, min_periods=1).mean()
+    return df
+
+
+def multi_ema(df: pd.DataFrame, spans: List[int], price_col: str = "close") -> pd.DataFrame:
+    df = df.copy()
+    for s in spans:
+        df[f"ema_{s}"] = ema(df, span=s, col=price_col)
+    return df
+
+
+def interaction_features(df: pd.DataFrame, left_cols: List[str], right_cols: List[str]) -> pd.DataFrame:
+    """Create pairwise interaction columns left * right if both present."""
+    df = df.copy()
+    for l in left_cols:
+        for r in right_cols:
+            if l in df.columns and r in df.columns:
+                name = f"{l}_x_{r}"
+                df[name] = df[l] * df[r]
+    return df
+
+
+def build_feature_matrix(df: pd.DataFrame, compute_list: Optional[List[str]] = None) -> pd.DataFrame:
+    """
+    Build a wide feature DataFrame from available columns.
+    compute_list: list of features to compute; if None compute defaults.
+    """
+    compute_list = compute_list or [
+        "ema_multi",
+        "vwap_20",
+        "atr_14",
+        "bb_20",
+        "macd",
+        "obv",
+        "vvr_20",
+        "vol_of_vol_20",
+        "candles",
+        "pv_corr_20",
+        "adv_1440",
+    ]
+    out = df.copy()
+    # ensure log_return exists if possible
+    if "log_return" not in out.columns and "close" in out.columns:
+        out["log_return"] = np.log(out["close"]).diff()
+
+    if "ema_multi" in compute_list:
+        out = multi_ema(out, spans=[5, 10, 20, 50, 100, 200])
+    if "vwap_20" in compute_list:
+        out = rolling_vwap(out, window=20)
+    if "atr_14" in compute_list:
+        out = atr(out, window=14)
+    if "bb_20" in compute_list:
+        out = bollinger_bands(out, window=20)
+    if "macd" in compute_list:
+        out = macd(out)
+    if "obv" in compute_list:
+        out = obv(out)
+    if "vvr_20" in compute_list:
+        out = vvr(out, vol_window=20)
+    if "vol_of_vol_20" in compute_list:
+        out = vol_of_vol(out, ret_col="log_return", window=20)
+    if "candles" in compute_list:
+        out = candle_morphology(out)
+    if "pv_corr_20" in compute_list:
+        out = rolling_price_volume_corr(out, window=20)
+    if "adv_1440" in compute_list:
+        out = adv(out, window=1440)
+
+    # interaction features example: RSI (if present) x ATR
+    left = [c for c in ["rsi", "atr", "vvr", "adv"] if c in out.columns]
+    right = [c for c in ["ema_20", "ema_50", "vol_of_vol"] if c in out.columns]
+    out = interaction_features(out, left_cols=left, right_cols=right)
+
+    # drop intermediate columns that are clearly non-feature (like timestamp)
+    # keep timestamp if present
+    return out
+
+
+def feature_report(df: pd.DataFrame, target_col: Optional[str] = None, out_dir: str = "backend/reports") -> dict:
+    """
+    Compute correlation matrix and feature importances.
+    If target_col provided, attempt to compute feature importances with XGBoost or RandomForest.
+    Returns dict with keys: correlation (DataFrame), importances (DataFrame or dict)
+    """
+    os.makedirs(out_dir, exist_ok=True)
+    features = df.select_dtypes(include=[np.number]).copy()
+    if features.empty:
+        return {"error": "no numeric features"}
+
+    corr = features.corr()
+    corr_path = os.path.join(out_dir, "feature_correlation.csv")
+    corr.to_csv(corr_path)
+
+    importances = None
+    if target_col and target_col in df.columns:
+        # prepare supervised target: if continuous, discretize to 3 classes (down/side/up) as example
+        y = df[target_col].dropna()
+        X = features.loc[y.index].fillna(0)
+        if len(y.unique()) > 10 and np.issubdtype(y.dtype, np.floating):
+            # discretize by quantiles to three classes
+            y_bins = pd.qcut(y, q=[0, 0.33, 0.66, 1.0], labels=[0, 1, 2], duplicates="drop")
+            y_used = y_bins.astype(int)
+        else:
+            y_used = y
+
+        # try XGBoost first
+        try:
+            if xgb is not None:
+                dtrain = xgb.DMatrix(X, label=y_used)
+                params = {"objective": "multi:softprob" if len(np.unique(y_used)) > 2 else "binary:logistic", "verbosity": 0}
+                num_class = len(np.unique(y_used))
+                if num_class > 2:
+                    params["num_class"] = num_class
+                bst = xgb.train(params, dtrain, num_boost_round=50)
+                imp = bst.get_score(importance_type="gain")
+                imp_df = pd.DataFrame(list(imp.items()), columns=["feature", "importance"]).sort_values("importance", ascending=False)
+                importances = imp_df
+            else:
+                raise ImportError("xgboost not installed")
+        except Exception:
+            # fallback to RandomForest
+            rf = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=42)
+            # if multiclass label may be categorical
+            try:
+                rf.fit(X, y_used)
+                imp = rf.feature_importances_
+                imp_df = pd.DataFrame({"feature": X.columns, "importance": imp}).sort_values("importance", ascending=False)
+                importances = imp_df
+            except Exception as e:
+                importances = {"error": str(e)}
+
+        # save importances
+        try:
+            if isinstance(importances, pd.DataFrame):
+                importances.to_csv(os.path.join(out_dir, "feature_importances.csv"), index=False)
+        except Exception:
+            pass
+
+    return {"correlation_csv": corr_path, "importances": importances}
+
+
+if __name__ == "__main__":
+    # quick smoke test
+    import argparse
+
+    p = argparse.ArgumentParser()
+    p.add_argument("--csv", required=True)
+    p.add_argument("--out", default="backend/reports")
+    args = p.parse_args()
+    df = pd.read_csv(args.csv)
+    features = build_feature_matrix(df)
+    report = feature_report(features, target_col="log_return", out_dir=args.out)
+    print("Saved correlation to", report.get("correlation_csv"))
+    print("Importances head:", getattr(report.get("importances"), "head", lambda: report.get("importances"))())
*** End Patch
*** Begin Patch
*** Add File: backend/binance_client.py
+import os
+import pandas as pd
+from binance.client import Client
+
+class BinanceClient:
+    def __init__(self, api_key=None, api_secret=None, testnet=False):
+        api_key = api_key or os.getenv("BINANCE_API_KEY")
+        api_secret = api_secret or os.getenv("BINANCE_API_SECRET")
+        if testnet:
+            api_key = api_key or os.getenv("BINANCE_TESTNET_API_KEY")
+            api_secret = api_secret or os.getenv("BINANCE_TESTNET_API_SECRET")
+        # Client will accept None for public endpoints, but for private calls keys are required
+        self.client = Client(api_key, api_secret)
+
+    def get_historical_klines(self, symbol, start_str, end_str, interval='1m') -> pd.DataFrame:
+        """
+        Returns a DataFrame with columns timestamp, open, high, low, close, volume
+        start_str and end_str are strings like '2024-11-01' or '1 day ago UTC'
+        """
+        raw = self.client.get_historical_klines(symbol, interval, start_str, end_str)
+        cols = ['open_time','open','high','low','close','volume','close_time','qav','num_trades','taker_base_vol','taker_quote_vol','ignore']
+        df = pd.DataFrame(raw, columns=cols)
+        df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')
+        df[['open','high','low','close','volume']] = df[['open','high','low','close','volume']].astype(float)
+        return df[['timestamp','open','high','low','close','volume']]
+
+    # Future: implement testnet create_test_order and live order wrappers as needed.
*** End Patch
*** Begin Patch
*** Add File: backend/model/__init__.py
+# model package
+__all__ = ["infer", "lora_train"]
*** End Patch
*** Begin Patch
*** Add File: backend/model/infer.py
+import os
+import warnings
+from typing import List, Dict
+
+import torch
+from transformers import AutoTokenizer, AutoModelForCausalLM
+try:
+    from transformers import BitsAndBytesConfig
+except Exception:
+    BitsAndBytesConfig = None
+
+MODEL = os.getenv("MODEL_REPO", "meta-llama/Llama-2-7b-chat-hf")
+
+def load_model():
+    """
+    Attempt to load a quantised model using bitsandbytes. Fallback to CPU model if needed.
+    """
+    try:
+        if BitsAndBytesConfig is not None:
+            # use quantisation config when available
+            bnb_config = BitsAndBytesConfig(load_in_8bit=True)
+            tokenizer = AutoTokenizer.from_pretrained(MODEL)
+            model = AutoModelForCausalLM.from_pretrained(MODEL, device_map='auto', quantization_config=bnb_config)
+            return tokenizer, model
+        else:
+            # bitsandbytes not available, try standard 8-bit argument
+            tokenizer = AutoTokenizer.from_pretrained(MODEL)
+            model = AutoModelForCausalLM.from_pretrained(MODEL, device_map='auto', load_in_8bit=True)
+            return tokenizer, model
+    except Exception as e:
+        warnings.warn(f"8-bit load failed: {e}. Falling back to cpu load.")
+        tokenizer = AutoTokenizer.from_pretrained(MODEL)
+        model = AutoModelForCausalLM.from_pretrained(MODEL, device_map='cpu')
+        return tokenizer, model
+
+def build_prompt(records: List[Dict]) -> str:
+    snippet = ""
+    for r in records[-40:]:
+        t = r.get('timestamp')
+        snippet += f"{t} O:{r.get('open')} H:{r.get('high')} L:{r.get('low')} C:{r.get('close')} V:{r.get('volume')} RSI:{r.get('rsi')} EMA50:{r.get('ema_50')} EMA200:{r.get('ema_200')}\n"
+    prompt = (
+        "You are an assistant specialising in short term crypto price analysis.\n"
+        "Use the supplied timestamped OHLCV and indicators to detect signals: EMA crossovers, RSI boundaries, momentum shifts and candle clusters.\n"
+        "Answer briefly with numbered bullet points: signals, trend, confidence and suggested next step for paper trading.\n\n"
+        f"{snippet}\nAnswer:"
+    )
+    return prompt
+
+def analyse_series(records: List[Dict]) -> str:
+    tokenizer, model = load_model()
+    prompt = build_prompt(records)
+    inputs = tokenizer(prompt, return_tensors='pt', truncation=True)
+    device = next(model.parameters()).device
+    # move inputs to device if not cpu
+    inputs = {k: v.to(device) for k, v in inputs.items()}
+    out = model.generate(**inputs, max_new_tokens=256, do_sample=False)
+    txt = tokenizer.decode(out[0], skip_special_tokens=True)
+    return txt
+
+if __name__ == "__main__":
+    # quick interactive test if run directly
+    import json
+    sample = [
+        {"timestamp":"2025-01-01 00:00","open":100,"high":102,"low":99,"close":101,"volume":1.2,"rsi":45,"ema_50":100,"ema_200":98}
+    ]
+    print(analyse_series(sample))
*** End Patch
*** Begin Patch
*** Add File: backend/model/lora_train.py
+import os
+import json
+import logging
+
+from datasets import load_dataset
+from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
+from peft import LoraConfig, get_peft_model
+
+MODEL = os.getenv("MODEL_REPO", "meta-llama/Llama-2-7b-chat-hf")
+
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+def _load_json_dataset(path: str):
+    """
+    Load a simple json file containing a list of {"prompt": "...", "response":"..."} pairs.
+    """
+    return load_dataset('json', data_files=path, split='train')
+
+def train_lora(dataset_path: str, output_dir: str):
+    # dataset: json file with {prompt, response}
+    ds = _load_json_dataset(dataset_path)
+    tokenizer = AutoTokenizer.from_pretrained(MODEL)
+
+    def tokenize_fn(ex):
+        full = ex['prompt'] + "\n" + ex['response']
+        enc = tokenizer(full, truncation=True, max_length=512)
+        enc['labels'] = enc['input_ids'].copy()
+        return enc
+
+    tokenized = ds.map(tokenize_fn, batched=False)
+
+    # load model in 8-bit if possible; Trainer will handle device_map with accelerate
+    try:
+        model = AutoModelForCausalLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=True)
+    except Exception as e:
+        logger.warning(f"8-bit load failed: {e}. Falling back to cpu load.")
+        model = AutoModelForCausalLM.from_pretrained(MODEL, device_map="cpu")
+
+    peft_config = LoraConfig(
+        task_type="CAUSAL_LM",
+        inference_mode=False,
+        r=8,
+        lora_alpha=32,
+        lora_dropout=0.1
+    )
+    model = get_peft_model(model, peft_config)
+
+    training_args = TrainingArguments(
+        per_device_train_batch_size=1,
+        num_train_epochs=1,
+        learning_rate=2e-4,
+        output_dir=output_dir,
+        logging_steps=10,
+        save_total_limit=2,
+        fp16=False
+    )
+
+    trainer = Trainer(
+        model=model,
+        args=training_args,
+        train_dataset=tokenized
+    )
+
+    trainer.train()
+    model.save_pretrained(output_dir)
+    logger.info("Saved LoRA adapter to %s", output_dir)
+
+if __name__ == "__main__":
+    # local quick test: create a tiny dataset if not present
+    sample_path = os.path.join("backend", "samples", "toy_train.json")
+    if not os.path.exists(sample_path):
+        os.makedirs(os.path.dirname(sample_path), exist_ok=True)
+        sample = [
+            {"prompt":"Price sequence: 2025-01-01 O:100 H:102 L:99 C:101 RSI:45 EMA50:100 EMA200:98\nQuestion: What pattern do you see?","response":"Short term uptrend, bullish crossover, RSI neutral. Bias: small buy with stop under 99."}
+        ]
+        with open(sample_path, "w") as fh:
+            json.dump(sample, fh)
+    train_lora(sample_path, "backend/model/checkpoints")
*** End Patch
*** Begin Patch
*** Add File: backend/backtest/__init__.py
+# backtest package
+__all__ = ["engine", "metrics"]
*** End Patch
*** Begin Patch
*** Add File: backend/backtest/engine.py
+import pandas as pd
+from .metrics import compute_metrics
+
+def simple_ma_rsi_strategy(df: pd.DataFrame):
+    """
+    Example strategy:
+    - Buy when EMA50 crosses above EMA200 and RSI < 70
+    - Sell when EMA50 crosses below EMA200 or RSI > 70
+    This is a naive backtest for demonstration.
+    """
+    df = df.copy().reset_index(drop=True)
+    df['position'] = 0
+    for i in range(1, len(df)):
+        prev = df.loc[i-1]
+        cur = df.loc[i]
+        buy_signal = (prev.get('ema_50') <= prev.get('ema_200')) and (cur.get('ema_50') > cur.get('ema_200')) and (cur.get('rsi') is not None and cur.get('rsi') < 70)
+        sell_signal = ((prev.get('ema_50') >= prev.get('ema_200')) and (cur.get('ema_50') < cur.get('ema_200'))) or (cur.get('rsi') is not None and cur.get('rsi') > 70)
+        if buy_signal:
+            df.at[i, 'position'] = 1
+        elif sell_signal:
+            df.at[i, 'position'] = 0
+        else:
+            df.at[i, 'position'] = df.at[i-1, 'position']
+
+    # compute returns based on close price change
+    df['close_shift'] = df['close'].shift(1)
+    df['strategy_ret'] = 0.0
+    mask = df['close_shift'] != 0
+    df.loc[mask, 'strategy_ret'] = ((df.loc[mask, 'close'] - df.loc[mask, 'close_shift']) / df.loc[mask, 'close_shift']) * df.loc[mask, 'position']
+    return compute_metrics(df['strategy_ret'])
+
+if __name__ == "__main__":
+    import pandas as pd
+    import numpy as np
+    # quick demo
+    df = pd.DataFrame({
+        'close': 100 + (np.arange(200) * 0.1),
+        'ema_50': 100 + (np.arange(200) * 0.09),
+        'ema_200': 100 + (np.arange(200) * 0.05),
+        'rsi': [50]*200
+    })
+    print(simple_ma_rsi_strategy(df))
*** End Patch
*** Begin Patch
*** Add File: backend/backtest/metrics.py
+import pandas as pd
+
+def compute_metrics(returns: pd.Series):
+    """
+    Compute simple performance metrics.
+    returns: a pd.Series of periodic returns (e.g. minute returns)
+    """
+    # total return
+    total_ret = (1 + returns).prod() - 1 if len(returns) > 0 else 0.0
+
+    # approximate annualised return and sharpe for minute data:
+    # periods per day = 1440
+    if len(returns) > 1:
+        avg_ret = returns.mean()
+        std_ret = returns.std(ddof=0) if returns.std(ddof=0) != 0 else 1e-8
+        annual_factor = 1440  # minutes per day used as simple scaler
+        sharpe = (avg_ret * annual_factor) / (std_ret * (annual_factor ** 0.5))
+    else:
+        sharpe = 0.0
+
+    # max drawdown
+    cum = (1 + returns).cumprod()
+    peak = cum.cummax()
+    dd = ((cum - peak) / peak).min() if len(cum) > 0 else 0.0
+
+    return {
+        "total_return": float(total_ret),
+        "sharpe": float(sharpe),
+        "max_drawdown": float(dd)
+    }
*** End Patch
*** Begin Patch
*** Add File: backend/tests/test_loader.py
+import os
+from backend.data.loader import load_csv
+
+def test_load_sample(tmp_path):
+    path = tmp_path / "sample_minute.csv"
+    import pandas as pd
+    import numpy as np
+    df = pd.DataFrame({
+        'timestamp': pd.date_range('2025-01-01', periods=10, freq='T'),
+        'open': (100 + np.arange(10)).astype(float),
+        'high': (101 + np.arange(10)).astype(float),
+        'low': (99 + np.arange(10)).astype(float),
+        'close': (100.5 + np.arange(10)).astype(float),
+        'volume': (1.0 + np.random.rand(10))
+    })
+    df.to_csv(path, index=False)
+    df_loaded = load_csv(str(path))
+    assert 'rsi' in df_loaded.columns
+    assert len(df_loaded) == 10
*** End Patch
*** Begin Patch
*** Add File: backend/tests/test_binance_client.py
+from backend.binance_client import BinanceClient
+
+def test_binance_client_init():
+    c = BinanceClient(api_key=None, api_secret=None)
+    assert c is not None
+
+def test_get_historical_klines_public_call():
+    """
+    This test does not call the network. It verifies the method can be accessed.
+    For an integration test, supply real API keys and run manually.
+    """
+    c = BinanceClient(api_key=None, api_secret=None)
+    assert hasattr(c, "get_historical_klines")
*** End Patch
*** Begin Patch
*** Add File: backend/samples/toy_train.json
+[
+  {"prompt":"Price sequence: 2025-01-01 O:100 H:102 L:99 C:101 RSI:45 EMA50:100 EMA200:98\nQuestion: What pattern do you see?","response":"Short term uptrend, bullish crossover, RSI neutral. Bias: small buy with stop under 99."},
+  {"prompt":"Price sequence: 2025-01-02 O:120 H:121 L:118 C:119 RSI:78 EMA50:110 EMA200:115\nQuestion: What pattern do you see?","response":"Overbought on RSI with price above EMAs. Risk of pullback. Consider taking profit."}
+]
*** End Patch
*** Begin Patch
*** Add File: backend/samples/prompt_templates.md
+System instruction:
+You are an assistant specialised in short term crypto price analysis. Use the supplied table of timestamped OHLCV rows and indicators to detect common signals: EMA crossovers, RSI >70 or <30, momentum shifts and candle clusters. Answer briefly and list signals and confidence.
+
+User prompt:
+{series_snippet}
+
+Answer:
+1. Signals:
+2. Trend:
+3. Confidence (low/medium/high):
+4. Suggested next step (paper trading only):
+
+Use concise bullet points and do not hallucinate numerical facts not present in the snippet.
*** End Patch
*** Begin Patch
*** Add File: backend/model/checkpoints/.gitkeep
+# placeholder to keep checkpoints directory in git if desired)
*** End Patch
*** Begin Patch
*** Add File: backend/data/README_IGNORE.txt
+The backend data folder contains loader, features, and audit helpers for the project.
+This file is intended to help keep the directory visible in some tools; ignore for production.
*** End Patch
*** End Patch
